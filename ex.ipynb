{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:17:34.216006Z",
     "start_time": "2023-07-25T17:17:33.315472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.17 (main, Jun 20 2023, 17:20:08) \n",
      "[Clang 14.0.3 (clang-1403.0.22.14.1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T17:17:40.468290Z",
     "start_time": "2023-07-25T17:17:33.336831Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:585: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [3], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mK\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/__init__.py:41\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/__init__.py:46\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tensorflow \u001B[38;5;28;01mas\u001B[39;00m _pywrap_tensorflow\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# pylint: enable=wildcard-import\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Bring in subpackages.\u001B[39;00m\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# from tensorflow.python import keras\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/__init__.py:25\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AUTOTUNE\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py:97\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m---> 97\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m service\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_ragged_batch\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_sparse_batch\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py:353\u001B[0m\n\u001B[1;32m    350\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m division\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m--> 353\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_dataset_id\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_dataset\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:26\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compression_ops\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute_options\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoShardPolicy\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute_options\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExternalStatePolicy\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:20\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m division\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m structure\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_experimental_dataset_ops \u001B[38;5;28;01mas\u001B[39;00m ged_ops\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompress\u001B[39m(element):\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:26\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwrapt\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:40\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse_tensor \u001B[38;5;28;01mas\u001B[39;00m _sparse_tensor\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py:28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constant_op\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m types_pb2\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m execute\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m op_callbacks\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tfe\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:585\u001B[0m\n\u001B[1;32m    556\u001B[0m     _NP_TO_TF[pdt] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m    557\u001B[0m         _NP_TO_TF[dt] \u001B[38;5;28;01mfor\u001B[39;00m dt \u001B[38;5;129;01min\u001B[39;00m _NP_TO_TF \u001B[38;5;28;01mif\u001B[39;00m dt \u001B[38;5;241m==\u001B[39m pdt()\u001B[38;5;241m.\u001B[39mdtype)  \u001B[38;5;66;03m# pylint: disable=no-value-for-parameter\u001B[39;00m\n\u001B[1;32m    559\u001B[0m TF_VALUE_DTYPES \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(_NP_TO_TF\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[1;32m    561\u001B[0m _TF_TO_NP \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    562\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_HALF:\n\u001B[1;32m    563\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[1;32m    564\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_FLOAT:\n\u001B[1;32m    565\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[1;32m    566\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_DOUBLE:\n\u001B[1;32m    567\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[1;32m    568\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT32:\n\u001B[1;32m    569\u001B[0m         np\u001B[38;5;241m.\u001B[39mint32,\n\u001B[1;32m    570\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT8:\n\u001B[1;32m    571\u001B[0m         np\u001B[38;5;241m.\u001B[39muint8,\n\u001B[1;32m    572\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT16:\n\u001B[1;32m    573\u001B[0m         np\u001B[38;5;241m.\u001B[39muint16,\n\u001B[1;32m    574\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT32:\n\u001B[1;32m    575\u001B[0m         np\u001B[38;5;241m.\u001B[39muint32,\n\u001B[1;32m    576\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT64:\n\u001B[1;32m    577\u001B[0m         np\u001B[38;5;241m.\u001B[39muint64,\n\u001B[1;32m    578\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT16:\n\u001B[1;32m    579\u001B[0m         np\u001B[38;5;241m.\u001B[39mint16,\n\u001B[1;32m    580\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT8:\n\u001B[1;32m    581\u001B[0m         np\u001B[38;5;241m.\u001B[39mint8,\n\u001B[1;32m    582\u001B[0m     \u001B[38;5;66;03m# NOTE(touts): For strings we use np.object as it supports variable length\u001B[39;00m\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;66;03m# strings.\u001B[39;00m\n\u001B[1;32m    584\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_STRING:\n\u001B[0;32m--> 585\u001B[0m         \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobject\u001B[49m,\n\u001B[1;32m    586\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX64:\n\u001B[1;32m    587\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex64,\n\u001B[1;32m    588\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX128:\n\u001B[1;32m    589\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex128,\n\u001B[1;32m    590\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT64:\n\u001B[1;32m    591\u001B[0m         np\u001B[38;5;241m.\u001B[39mint64,\n\u001B[1;32m    592\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BOOL:\n\u001B[1;32m    593\u001B[0m         np\u001B[38;5;241m.\u001B[39mbool_,\n\u001B[1;32m    594\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT8:\n\u001B[1;32m    595\u001B[0m         _np_qint8,\n\u001B[1;32m    596\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT8:\n\u001B[1;32m    597\u001B[0m         _np_quint8,\n\u001B[1;32m    598\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT16:\n\u001B[1;32m    599\u001B[0m         _np_qint16,\n\u001B[1;32m    600\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT16:\n\u001B[1;32m    601\u001B[0m         _np_quint16,\n\u001B[1;32m    602\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT32:\n\u001B[1;32m    603\u001B[0m         _np_qint32,\n\u001B[1;32m    604\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BFLOAT16:\n\u001B[1;32m    605\u001B[0m         _np_bfloat16,\n\u001B[1;32m    606\u001B[0m \n\u001B[1;32m    607\u001B[0m     \u001B[38;5;66;03m# Ref types\u001B[39;00m\n\u001B[1;32m    608\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_HALF_REF:\n\u001B[1;32m    609\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[1;32m    610\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_FLOAT_REF:\n\u001B[1;32m    611\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[1;32m    612\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_DOUBLE_REF:\n\u001B[1;32m    613\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[1;32m    614\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT32_REF:\n\u001B[1;32m    615\u001B[0m         np\u001B[38;5;241m.\u001B[39mint32,\n\u001B[1;32m    616\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT32_REF:\n\u001B[1;32m    617\u001B[0m         np\u001B[38;5;241m.\u001B[39muint32,\n\u001B[1;32m    618\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT8_REF:\n\u001B[1;32m    619\u001B[0m         np\u001B[38;5;241m.\u001B[39muint8,\n\u001B[1;32m    620\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT16_REF:\n\u001B[1;32m    621\u001B[0m         np\u001B[38;5;241m.\u001B[39muint16,\n\u001B[1;32m    622\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT16_REF:\n\u001B[1;32m    623\u001B[0m         np\u001B[38;5;241m.\u001B[39mint16,\n\u001B[1;32m    624\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT8_REF:\n\u001B[1;32m    625\u001B[0m         np\u001B[38;5;241m.\u001B[39mint8,\n\u001B[1;32m    626\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_STRING_REF:\n\u001B[1;32m    627\u001B[0m         np\u001B[38;5;241m.\u001B[39mobject,\n\u001B[1;32m    628\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX64_REF:\n\u001B[1;32m    629\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex64,\n\u001B[1;32m    630\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX128_REF:\n\u001B[1;32m    631\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex128,\n\u001B[1;32m    632\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT64_REF:\n\u001B[1;32m    633\u001B[0m         np\u001B[38;5;241m.\u001B[39mint64,\n\u001B[1;32m    634\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT64_REF:\n\u001B[1;32m    635\u001B[0m         np\u001B[38;5;241m.\u001B[39muint64,\n\u001B[1;32m    636\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BOOL_REF:\n\u001B[1;32m    637\u001B[0m         np\u001B[38;5;241m.\u001B[39mbool,\n\u001B[1;32m    638\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT8_REF:\n\u001B[1;32m    639\u001B[0m         _np_qint8,\n\u001B[1;32m    640\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT8_REF:\n\u001B[1;32m    641\u001B[0m         _np_quint8,\n\u001B[1;32m    642\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT16_REF:\n\u001B[1;32m    643\u001B[0m         _np_qint16,\n\u001B[1;32m    644\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT16_REF:\n\u001B[1;32m    645\u001B[0m         _np_quint16,\n\u001B[1;32m    646\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT32_REF:\n\u001B[1;32m    647\u001B[0m         _np_qint32,\n\u001B[1;32m    648\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BFLOAT16_REF:\n\u001B[1;32m    649\u001B[0m         _np_bfloat16,\n\u001B[1;32m    650\u001B[0m }\n\u001B[1;32m    652\u001B[0m _QUANTIZED_DTYPES_NO_REF \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m([qint8, quint8, qint16, quint16, qint32])\n\u001B[1;32m    653\u001B[0m _QUANTIZED_DTYPES_REF \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m(\n\u001B[1;32m    654\u001B[0m     [qint8_ref, quint8_ref, qint16_ref, quint16_ref, qint32_ref])\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/numpy/__init__.py:313\u001B[0m, in \u001B[0;36m__getattr__\u001B[0;34m(attr)\u001B[0m\n\u001B[1;32m    308\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    309\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn the future `np.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` will be defined as the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    310\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorresponding NumPy scalar.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;129;01min\u001B[39;00m __former_attrs__:\n\u001B[0;32m--> 313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(__former_attrs__[attr])\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtesting\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtesting\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtesting\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import wandb\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "import random\n",
    "from glob import glob\n",
    "import IPython.display as ipd\n",
    "from tqdm.notebook import tqdm\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Show less log messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/Users/morzahavi/Library/Mobile Documents/com~apple~CloudDocs/Downloads/asvspoof/LA'\n",
    "FOLDS = 10\n",
    "SEED = 101\n",
    "DEBUG = True\n",
    "\n",
    "# Audio params\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5.0 # duration in second\n",
    "AUDIO_LEN = int(SAMPLE_RATE * DURATION)\n",
    "\n",
    "# Spectrogram params\n",
    "N_MELS = 128 # freq axis\n",
    "N_FFT = 2048\n",
    "SPEC_WIDTH = 256 # time axis\n",
    "HOP_LEN = AUDIO_LEN//(SPEC_WIDTH - 1) # non-overlap region\n",
    "FMAX = SAMPLE_RATE//2 # max frequency\n",
    "SPEC_SHAPE = [SPEC_WIDTH, N_MELS] # output spectrogram shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt',\n",
    "                       sep=\" \", header=None)\n",
    "train_df.columns =['speaker_id','filename','system_id','null','class_name']\n",
    "train_df.drop(columns=['null'],inplace=True)\n",
    "train_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_train/flac/'+train_df.filename+'.flac'\n",
    "train_df['target'] = (train_df.class_name=='spoof').astype('int32') # set labels 1 for fake and 0 for real\n",
    "if DEBUG:\n",
    "    train_df = train_df.groupby(['target']).sample(2500).reset_index(drop=True)\n",
    "print(f'Train Samples: {len(train_df)}')\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt',\n",
    "                       sep=\" \", header=None)\n",
    "valid_df.columns =['speaker_id','filename','system_id','null','class_name']\n",
    "valid_df.drop(columns=['null'],inplace=True)\n",
    "valid_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_dev/flac/'+valid_df.filename+'.flac'\n",
    "valid_df['target'] = (valid_df.class_name=='spoof').astype('int32')\n",
    "if DEBUG:\n",
    "    valid_df = valid_df.groupby(['target']).sample(2000).reset_index(drop=True)\n",
    "print(f'Valid Samples: {len(valid_df)}')\n",
    "valid_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt',\n",
    "                      sep=\" \", header=None)\n",
    "test_df.columns =['speaker_id','filename','system_id','null','class_name']\n",
    "test_df.drop(columns=['null'],inplace=True)\n",
    "test_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_eval/flac/'+test_df.filename+'.flac'\n",
    "test_df['target'] = (test_df.class_name=='spoof').astype('int32')\n",
    "if DEBUG:\n",
    "    test_df = test_df.groupby(['target']).sample(2000).reset_index(drop=True)\n",
    "print(f'Test Samples: {len(test_df)}')\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr=16000):\n",
    "    \"\"\"load audio from .wav file\n",
    "    Args:\n",
    "        path: file path of .wav file\n",
    "        sr: sample rate\n",
    "    Returns:\n",
    "        audio, sr\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(path, sr=sr)\n",
    "    return audio, sr\n",
    "\n",
    "def plot_audio(audio, sr=16000):\n",
    "    fig = librosa.display.waveshow(audio,\n",
    "                                   x_axis='time',\n",
    "                                   sr=sr)\n",
    "    return fig\n",
    "\n",
    "def listen_audio(audio, sr=16000):\n",
    "    display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "def get_spec(audio):\n",
    "    spec = librosa.feature.melspectrogram(audio, fmax=FMAX, n_mels=N_MELS, hop_length=HOP_LEN, n_fft=N_FFT)\n",
    "    spec = librosa.power_to_db(spec)\n",
    "    return spec\n",
    "\n",
    "def plot_spec(spec, sr=16000):\n",
    "    fig = librosa.display.specshow(spec,\n",
    "                                   x_axis='time',\n",
    "                                   y_axis='hz',\n",
    "                                   hop_length=HOP_LEN,\n",
    "                                   sr=SAMPLE_RATE,\n",
    "                                   fmax=FMAX,)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = train_df[train_df.target==0].iloc[10]\n",
    "print(f'> Filename: {row.filename} | Label: {row.class_name}')\n",
    "audio, sr= load_audio(row.filepath, sr=None)\n",
    "audio = audio[:AUDIO_LEN]\n",
    "spec = get_spec(audio)\n",
    "\n",
    "print('# Listen')\n",
    "listen_audio(audio, sr=16000)\n",
    "\n",
    "print(\"# Plot\\n\")\n",
    "plt.figure(figsize=(12*2,5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.subplot(121)\n",
    "plot_audio(audio)\n",
    "plt.title(\"Waveform\",fontsize=17)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_spec(spec);\n",
    "plt.title(\"Spectrogram\",fontsize=17)\n",
    "\n",
    "#\n",
    "# aspect_ratio = 10\n",
    "# width = 8.0\n",
    "# height = width / aspect_ratio\n",
    "plt.tight_layout()\n",
    "plt.savefig('wave_spect')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Split train data into folds\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train_df, y=train_df['target'])):\n",
    "    train_df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "# Split valid data into folds\n",
    "for fold, (_, val_idx) in enumerate(skf.split(valid_df, y=valid_df['target'])):\n",
    "    valid_df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "# Split test data into folds\n",
    "for fold, (_, val_idx) in enumerate(skf.split(test_df, y=test_df['target'])):\n",
    "    test_df.loc[val_idx, 'fold'] = fold\n",
    "display(test_df.groupby(['fold','target']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_serialize_example(feature0, feature1, feature2,\n",
    "                            feature3, feature4,feature5,feature6):\n",
    "    feature = {\n",
    "        'audio':_bytes_feature(feature0),\n",
    "        'id':_bytes_feature(feature1),\n",
    "        'speaker_id':_bytes_feature(feature2),\n",
    "        'system_id':_bytes_feature(feature3),\n",
    "        'class_name':_bytes_feature(feature4),\n",
    "        'audio_len':_int64_feature(feature5),\n",
    "        'target':_int64_feature(feature6),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/tmp/asvspoof', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(df, split='train', show=True):\n",
    "    df = df.copy()\n",
    "    folds = sorted(df.fold.unique().tolist())\n",
    "    for fold in tqdm(folds): # create tfrecord for each fold\n",
    "        fold_df = df.query(\"fold==@fold\").sample(frac=1.0)\n",
    "        if show:\n",
    "            print(); print('Writing %s TFRecord of fold %i :'%(split,fold))\n",
    "        with tf.io.TFRecordWriter('/tmp/asvspoof/%s%.2i-%i.tfrec'%(split,fold,fold_df.shape[0])) as writer:\n",
    "            samples = fold_df.shape[0] # samples = 200\n",
    "            it = tqdm(range(samples)) if show else range(samples)\n",
    "            for k in it: # images in fold\n",
    "                row = fold_df.iloc[k,:]\n",
    "                audio, sr = load_audio(row['filepath'])\n",
    "                audio_id = row['filename']\n",
    "                speaker_id = row['speaker_id']\n",
    "                system_id = row['system_id']\n",
    "                class_name = row['class_name']\n",
    "                target = row['target']\n",
    "                example  = train_serialize_example(\n",
    "                    tf.audio.encode_wav(audio[...,None],sample_rate=sr),\n",
    "                    str.encode(audio_id),\n",
    "                    str.encode(speaker_id),\n",
    "                    str.encode(system_id),\n",
    "                    str.encode(class_name),\n",
    "                    len(audio),\n",
    "                    int(target),\n",
    "                )\n",
    "                writer.write(example)\n",
    "            if show:\n",
    "                filepath = '/tmp/asvspoof/%s%.2i-%i.tfrec'%(split,fold,fold_df.shape[0])\n",
    "                filename = filepath.split('/')[-1]\n",
    "                filesize = os.path.getsize(filepath)/10**6\n",
    "                print(filename,':',np.around(filesize, 2),'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_tfrecord(train_df,split='train', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_tfrecord(valid_df,split='valid', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_tfrecord(test_df,split='test', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "def decode_audio(data, audio_len, target_len=1*16000):\n",
    "    audio, sr = tf.audio.decode_wav(data)\n",
    "    audio = tf.reshape(audio, [audio_len]) # explicit size needed for TPU\n",
    "    audio = audio[:target_len]\n",
    "    audio = tf.cast(audio,tf.float32)\n",
    "    # compute min and max\n",
    "    min_ = tf.reduce_min(audio)\n",
    "    max_ = tf.reduce_max(audio)\n",
    "    # normalization\n",
    "    audio = (audio - min_) / (max_ - min_) # mean=0 & std=1\n",
    "    return audio\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"audio\" : tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"audio_len\" : tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"target\" : tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    audio_len = example['audio_len']\n",
    "    audio = decode_audio(example['audio'], audio_len)\n",
    "    target = example['target']\n",
    "    return audio, target  # returns a dataset of (image, label) pairs\n",
    "\n",
    "def load_dataset(fileids, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(fileids, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def get_dataset(FILENAMES):\n",
    "    dataset = load_dataset(FILENAMES, labeled=True)\n",
    "    #     dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(100, seed=SEED)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(fileids):\n",
    "    # the number of data items is written in the id of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(fileid).group(1)) for fileid in fileids]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch(batch, row=2, col=5):\n",
    "    audios, targets = batch\n",
    "    plt.figure(figsize=(col*5, 5*row))\n",
    "    for idx in range(row*col):\n",
    "        audio = audios[idx,]\n",
    "        target = targets[idx,]\n",
    "        plt.subplot(row, col, idx+1)\n",
    "        plt.plot(audio, color='r' if target else 'b')\n",
    "        plt.title('Fake' if target else 'Real', fontsize=15)\n",
    "    #         plt.xticks([])\n",
    "    #         plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()#%%\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_PATH = '~/icloud/Downloads/asvspoof/LA'\n",
    "FOLDS = 10\n",
    "SEED = 101\n",
    "DEBUG = True\n",
    "\n",
    "# Audio params\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5.0 # duration in second\n",
    "AUDIO_LEN = int(SAMPLE_RATE * DURATION)\n",
    "\n",
    "# Spectrogram params\n",
    "N_MELS = 128 # freq axis\n",
    "N_FFT = 2048\n",
    "SPEC_WIDTH = 256 # time axis\n",
    "HOP_LEN = AUDIO_LEN//(SPEC_WIDTH - 1) # non-overlap region\n",
    "FMAX = SAMPLE_RATE//2 # max frequency\n",
    "SPEC_SHAPE = [SPEC_WIDTH, N_MELS] # output spectrogram shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt',\n",
    "                       sep=\" \", header=None)\n",
    "train_df.columns =['speaker_id','filename','system_id','null','class_name']\n",
    "train_df.drop(columns=['null'],inplace=True)\n",
    "train_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_train/flac/'+train_df.filename+'.flac'\n",
    "train_df['target'] = (train_df.class_name=='spoof').astype('int32') # set labels 1 for fake and 0 for real\n",
    "if DEBUG:\n",
    "    train_df = train_df.groupby(['target']).sample(2500).reset_index(drop=True)\n",
    "print(f'Train Samples: {len(train_df)}')\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt',\n",
    "                       sep=\" \", header=None)\n",
    "valid_df.columns =['speaker_id','filename','system_id','null','class_name']\n",
    "valid_df.drop(columns=['null'],inplace=True)\n",
    "valid_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_dev/flac/'+valid_df.filename+'.flac'\n",
    "valid_df['target'] = (valid_df.class_name=='spoof').astype('int32')\n",
    "if DEBUG:\n",
    "    valid_df = valid_df.groupby(['target']).sample(2000).reset_index(drop=True)\n",
    "print(f'Valid Samples: {len(valid_df)}')\n",
    "valid_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt',\n",
    "                      sep=\" \", header=None)\n",
    "test_df.columns =['speaker_id','filename','system_id','null','class_name']\n",
    "test_df.drop(columns=['null'],inplace=True)\n",
    "test_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_eval/flac/'+test_df.filename+'.flac'\n",
    "test_df['target'] = (test_df.class_name=='spoof').astype('int32')\n",
    "if DEBUG:\n",
    "    test_df = test_df.groupby(['target']).sample(2000).reset_index(drop=True)\n",
    "print(f'Test Samples: {len(test_df)}')\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr=16000):\n",
    "    \"\"\"load audio from .wav file\n",
    "    Args:\n",
    "        path: file path of .wav file\n",
    "        sr: sample rate\n",
    "    Returns:\n",
    "        audio, sr\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(path, sr=sr)\n",
    "    return audio, sr\n",
    "\n",
    "def plot_audio(audio, sr=16000):\n",
    "    fig = librosa.display.waveshow(audio,\n",
    "                                   x_axis='time',\n",
    "                                   sr=sr)\n",
    "    return fig\n",
    "\n",
    "def listen_audio(audio, sr=16000):\n",
    "    display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "def get_spec(audio):\n",
    "    spec = librosa.feature.melspectrogram(audio, fmax=FMAX, n_mels=N_MELS, hop_length=HOP_LEN, n_fft=N_FFT)\n",
    "    spec = librosa.power_to_db(spec)\n",
    "    return spec\n",
    "\n",
    "def plot_spec(spec, sr=16000):\n",
    "    fig = librosa.display.specshow(spec,\n",
    "                                   x_axis='time',\n",
    "                                   y_axis='hz',\n",
    "                                   hop_length=HOP_LEN,\n",
    "                                   sr=SAMPLE_RATE,\n",
    "                                   fmax=FMAX,)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listen_audio(LA_T_3685206.flac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = train_df[train_df.target==0].iloc[10]\n",
    "print(f'> Filename: {row.filename} | Label: {row.class_name}')\n",
    "audio, sr= load_audio(row.filepath, sr=None)\n",
    "audio = audio[:AUDIO_LEN]\n",
    "spec = get_spec(audio)\n",
    "\n",
    "print('# Listen')\n",
    "listen_audio(audio, sr=16000)\n",
    "\n",
    "print(\"# Plot\\n\")\n",
    "plt.figure(figsize=(12*2,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_audio(audio)\n",
    "plt.title(\"Waveform\",fontsize=17)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_spec(spec);\n",
    "plt.title(\"Spectrogram\",fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Split train data into folds\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train_df, y=train_df['target'])):\n",
    "    train_df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "# Split valid data into folds\n",
    "for fold, (_, val_idx) in enumerate(skf.split(valid_df, y=valid_df['target'])):\n",
    "    valid_df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "# Split test data into folds\n",
    "for fold, (_, val_idx) in enumerate(skf.split(test_df, y=test_df['target'])):\n",
    "    test_df.loc[val_idx, 'fold'] = fold\n",
    "display(test_df.groupby(['fold','target']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_serialize_example(feature0, feature1, feature2,\n",
    "                            feature3, feature4,feature5,feature6):\n",
    "    feature = {\n",
    "        'audio':_bytes_feature(feature0),\n",
    "        'id':_bytes_feature(feature1),\n",
    "        'speaker_id':_bytes_feature(feature2),\n",
    "        'system_id':_bytes_feature(feature3),\n",
    "        'class_name':_bytes_feature(feature4),\n",
    "        'audio_len':_int64_feature(feature5),\n",
    "        'target':_int64_feature(feature6),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/tmp/asvspoof', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(df, split='train', show=True):\n",
    "    df = df.copy()\n",
    "    folds = sorted(df.fold.unique().tolist())\n",
    "    for fold in tqdm(folds): # create tfrecord for each fold\n",
    "        fold_df = df.query(\"fold==@fold\").sample(frac=1.0)\n",
    "        if show:\n",
    "            print(); print('Writing %s TFRecord of fold %i :'%(split,fold))\n",
    "        with tf.io.TFRecordWriter('/tmp/asvspoof/%s%.2i-%i.tfrec'%(split,fold,fold_df.shape[0])) as writer:\n",
    "            samples = fold_df.shape[0] # samples = 200\n",
    "            it = tqdm(range(samples)) if show else range(samples)\n",
    "            for k in it: # images in fold\n",
    "                row = fold_df.iloc[k,:]\n",
    "                audio, sr = load_audio(row['filepath'])\n",
    "                audio_id = row['filename']\n",
    "                speaker_id = row['speaker_id']\n",
    "                system_id = row['system_id']\n",
    "                class_name = row['class_name']\n",
    "                target = row['target']\n",
    "                example  = train_serialize_example(\n",
    "                    tf.audio.encode_wav(audio[...,None],sample_rate=sr),\n",
    "                    str.encode(audio_id),\n",
    "                    str.encode(speaker_id),\n",
    "                    str.encode(system_id),\n",
    "                    str.encode(class_name),\n",
    "                    len(audio),\n",
    "                    int(target),\n",
    "                )\n",
    "                writer.write(example)\n",
    "            if show:\n",
    "                filepath = '/tmp/asvspoof/%s%.2i-%i.tfrec'%(split,fold,fold_df.shape[0])\n",
    "                filename = filepath.split('/')[-1]\n",
    "                filesize = os.path.getsize(filepath)/10**6\n",
    "                print(filename,':',np.around(filesize, 2),'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_tfrecord(train_df,split='train', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_tfrecord(valid_df,split='valid', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_tfrecord(test_df,split='test', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "def decode_audio(data, audio_len, target_len=1*16000):\n",
    "    audio, sr = tf.audio.decode_wav(data)\n",
    "    audio = tf.reshape(audio, [audio_len]) # explicit size needed for TPU\n",
    "    audio = audio[:target_len]\n",
    "    audio = tf.cast(audio,tf.float32)\n",
    "    # compute min and max\n",
    "    min_ = tf.reduce_min(audio)\n",
    "    max_ = tf.reduce_max(audio)\n",
    "    # normalization\n",
    "    audio = (audio - min_) / (max_ - min_) # mean=0 & std=1\n",
    "    return audio\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"audio\" : tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"audio_len\" : tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"target\" : tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    audio_len = example['audio_len']\n",
    "    audio = decode_audio(example['audio'], audio_len)\n",
    "    target = example['target']\n",
    "    return audio, target  # returns a dataset of (image, label) pairs\n",
    "\n",
    "def load_dataset(fileids, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(fileids, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def get_dataset(FILENAMES):\n",
    "    dataset = load_dataset(FILENAMES, labeled=True)\n",
    "    #     dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(100, seed=SEED)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(fileids):\n",
    "    # the number of data items is written in the id of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(fileid).group(1)) for fileid in fileids]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch(batch, row=2, col=5):\n",
    "    audios, targets = batch\n",
    "    plt.figure(figsize=(col*5, 5*row))\n",
    "    for idx in range(row*col):\n",
    "        audio = audios[idx,]\n",
    "        target = targets[idx,]\n",
    "        plt.subplot(row, col, idx+1)\n",
    "        plt.plot(audio, color='r' if target else 'b')\n",
    "        plt.title('Fake' if target else 'Real', fontsize=15)\n",
    "    #         plt.xticks([])\n",
    "    #         plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    BATCH_SIZE = 32\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    TRAIN_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/train*.tfrec')\n",
    "    VALID_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/valid*.tfrec')\n",
    "    TEST_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/test*.tfrec')\n",
    "    print('There are %i train, %i valid & %i test images'%(count_data_items(TRAIN_FILENAMES),\n",
    "                                                           count_data_items(VALID_FILENAMES),\n",
    "                                                           count_data_items(TEST_FILENAMES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Batch Images\n",
    "ds = get_dataset(TRAIN_FILENAMES)\n",
    "batch = next(iter(ds))\n",
    "display_batch(batch, row=2, col=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "TRAIN_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/train*.tfrec')\n",
    "VALID_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/valid*.tfrec')\n",
    "TEST_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/test*.tfrec')\n",
    "print('There are %i train, %i valid & %i test images'%(count_data_items(TRAIN_FILENAMES),\n",
    "                                                       count_data_items(VALID_FILENAMES),\n",
    "                                                       count_data_items(TEST_FILENAMES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb = True\n",
    "    project = \"fake-speech-detection\"\n",
    "    debug = False\n",
    "    exp_name = \"v0\"\n",
    "    comment = \"Conformer-128x80-cosine-no_aug-no_fc\"\n",
    "\n",
    "    # Use verbose=0 for silent, 1 for interactive\n",
    "    verbose = 0\n",
    "    display_plot = True\n",
    "\n",
    "    # Device for training\n",
    "    device = None  # device is automatically selected\n",
    "\n",
    "    # Model & Backbone\n",
    "    model_name = \"Conformer\"\n",
    "\n",
    "    # Seeding for reproducibility\n",
    "    seed = 101\n",
    "\n",
    "    # Audio params\n",
    "    sample_rate = 16000\n",
    "    duration = 3.5 # duration in second\n",
    "    audio_len = int(sample_rate * duration)\n",
    "    normalize = True\n",
    "\n",
    "    # Spectrogram params\n",
    "    spec_freq = 128 # freq axis\n",
    "    n_fft = 2048\n",
    "    spec_time = 256 # time axis\n",
    "    hop_len = audio_len//(spec_time - 1) # non-overlap region\n",
    "    fmin = 20\n",
    "    fmax = sample_rate//2 # max frequency\n",
    "    spec_shape = [spec_time, spec_freq] # output spectrogram shape\n",
    "\n",
    "    # Audio Augmentation\n",
    "    timeshift_prob = 0.0\n",
    "    gn_prob = 0.0\n",
    "\n",
    "    # Spectrogram Augmentation\n",
    "    time_mask = 20\n",
    "    freq_mask = 10\n",
    "    cutmix_prob = 0.0\n",
    "    cutmix_alpha = 2.5\n",
    "    mixup_prob = 0.0\n",
    "    mixup_alpha = 2.5\n",
    "\n",
    "    # Batch Size & Epochs\n",
    "    batch_size = 32\n",
    "    drop_remainder = False\n",
    "    epochs = 12\n",
    "    steps_per_execution = None\n",
    "\n",
    "    # Loss & Optimizer & LR Scheduler\n",
    "    loss = \"binary_crossentropy\"\n",
    "    optimizer = \"Adam\"\n",
    "    lr = 1e-4\n",
    "    lr_schedule = \"cosine\"\n",
    "\n",
    "    # Augmentation\n",
    "    augment = False\n",
    "\n",
    "    # Clip values to [0, 1]\n",
    "    clip = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(SEED):\n",
    "    \"\"\"\n",
    "    Sets all random seeds for the program (Python, NumPy, and TensorFlow).\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    #     os.environ[\"TF_CUDNN_DETERMINISTIC\"] = str(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"seeding done!!!\")\n",
    "\n",
    "\n",
    "seeding(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_device():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # connect to tpu cluster\n",
    "        strategy = tf.distribute.TPUStrategy(tpu) # get strategy for tpu\n",
    "        print('> Running on TPU ', tpu.master(), end=' | ')\n",
    "        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n",
    "        device='TPU'\n",
    "    except: # otherwise detect GPUs\n",
    "        tpu = None\n",
    "        gpus = tf.config.list_logical_devices('GPU') # get logical gpus\n",
    "        ngpu = len(gpus)\n",
    "        if ngpu: # if number of GPUs are 0 then CPU\n",
    "            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n",
    "            print(\"> Running on GPU\", end=' | ')\n",
    "            print(\"Num of GPUs: \", ngpu)\n",
    "            device='GPU'\n",
    "        else:\n",
    "            print(\"> Running on CPU\")\n",
    "            strategy = tf.distribute.get_strategy() # connect to single gpu or cpu\n",
    "            device='CPU'\n",
    "    return strategy, device, tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy, CFG.device, tpu = configure_device()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/Users/morzahavi/Library/Mobile Documents/com~apple~CloudDocs/Downloads/asvspoof/LA'\n",
    "\n",
    "# Train\n",
    "train_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt',\n",
    "                       sep=\" \", header=None)\n",
    "train_df.columns =['speaker_id','filename','system_id','null','class_name']\n",
    "train_df.drop(columns=['null'],inplace=True)\n",
    "train_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_train/flac/'+train_df.filename+'.flac'\n",
    "train_df['target'] = (train_df.class_name=='spoof').astype('int32') # set labels 1 for fake and 0 for real\n",
    "if True:\n",
    "    train_df = train_df.groupby(['target']).sample(2500).reset_index(drop=True)\n",
    "print(f'Train Samples: {len(train_df)}')\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_int(shape=[], minval=0, maxval=1):\n",
    "    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n",
    "\n",
    "def random_float(shape=[], minval=0.0, maxval=1.0):\n",
    "    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n",
    "    return rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim Audio to ignore silent part in the start and end\n",
    "def TrimAudio(audio, epsilon=0.15):\n",
    "    pos  = tfio.audio.trim(audio, axis=0, epsilon=epsilon)\n",
    "    audio = audio[pos[0]:pos[1]]\n",
    "    return audio\n",
    "\n",
    "# Crop or Pad audio to keep a fixed length\n",
    "def CropOrPad(audio, target_len, pad_mode='constant'):\n",
    "    audio_len = tf.shape(audio)[0]\n",
    "    if audio_len < target_len: # if audio_len is smaller than target_len then use Padding\n",
    "        diff_len = (target_len - audio_len)\n",
    "        pad1 = random_int([], minval=0, maxval=diff_len) # select random location for padding\n",
    "        pad2 = diff_len - pad1\n",
    "        pad_len = [pad1, pad2]\n",
    "        audio = tf.pad(audio, paddings=[pad_len], mode=pad_mode) # apply padding\n",
    "    elif audio_len > target_len:  # if audio_len is larger than target_len then use Cropping\n",
    "        diff_len = (audio_len - target_len)\n",
    "        idx = tf.random.uniform([], 0, diff_len, dtype=tf.int32) # select random location for cropping\n",
    "        audio = audio[idx: (idx + target_len)]\n",
    "    audio = tf.reshape(audio, [target_len])\n",
    "    return audio\n",
    "\n",
    "# Randomly shift audio -> any sound at <t> time may get shifted to <t+shift> time\n",
    "def TimeShift(audio, prob=0.5):\n",
    "    if random_float() < prob:\n",
    "        shift = random_int(shape=[], minval=0, maxval=tf.shape(audio)[0])\n",
    "        if random_float() < 0.5:\n",
    "            shift = -shift\n",
    "        audio = tf.roll(audio, shift, axis=0)\n",
    "    return audio\n",
    "\n",
    "# Apply random noise to audio data\n",
    "def GaussianNoise(audio, std=[0.0025, 0.025], prob=0.5):\n",
    "    std = random_float([], std[0], std[1])\n",
    "    if random_float() < prob:\n",
    "        GN = tf.keras.layers.GaussianNoise(stddev=std)\n",
    "        audio = GN(audio, training=True) # training=False don't apply noise to data\n",
    "    return audio\n",
    "\n",
    "# Applies augmentation to Audio Signal\n",
    "def AudioAug(audio):\n",
    "    audio = TimeShift(audio, prob=CFG.timeshift_prob)\n",
    "    audio = GaussianNoise(audio, prob=CFG.gn_prob)\n",
    "    return audio\n",
    "\n",
    "def Normalize(data):\n",
    "    MEAN = tf.math.reduce_mean(data)\n",
    "    STD = tf.math.reduce_std(data)\n",
    "    data = tf.math.divide_no_nan(data - MEAN, STD)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly mask data in time and freq axis\n",
    "def TimeFreqMask(spec, time_mask, freq_mask, prob=0.5):\n",
    "    if random_float() < prob:\n",
    "        spec = tfio.audio.freq_mask(spec, param=freq_mask)\n",
    "        spec = tfio.audio.time_mask(spec, param=time_mask)\n",
    "    return spec\n",
    "\n",
    "# Applies augmentation to Spectrogram\n",
    "def SpecAug(spec):\n",
    "    spec = TimeFreqMask(spec, time_mask=CFG.time_mask, freq_mask=CFG.freq_mask, prob=0.5)\n",
    "    return spec\n",
    "\n",
    "# Compute MixUp Augmentation for Spectrogram\n",
    "def get_mixup(alpha=0.2, prob=0.5):\n",
    "    \"\"\"Apply Spectrogram-MixUp augmentaiton. Apply Mixup to one batch and its shifted version\"\"\"\n",
    "    def mixup(specs, labels, alpha=alpha, prob=prob):\n",
    "        if random_float() > prob:\n",
    "            return specs, labels\n",
    "\n",
    "        spec_shape = tf.shape(specs)\n",
    "        label_shape = tf.shape(labels)\n",
    "\n",
    "        beta = tfp.distributions.Beta(alpha, alpha) # select lambda from beta distribution\n",
    "        lam = beta.sample(1)[0]\n",
    "\n",
    "        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n",
    "        specs = lam * specs + (1 - lam) * tf.roll(specs, shift=1, axis=0) # mixup = [1, 2, 3]*lam + [3, 1, 2]*(1 - lam)\n",
    "        labels = lam * labels + (1 - lam) * tf.roll(labels, shift=1, axis=0)\n",
    "\n",
    "        specs = tf.reshape(specs, spec_shape)\n",
    "        labels = tf.reshape(labels, label_shape)\n",
    "        return specs, labels\n",
    "    return mixup\n",
    "\n",
    "\n",
    "def get_cutmix(alpha, prob=0.5):\n",
    "    \"\"\"Apply Spectrogram-CutMix augmentaiton which only cuts patch across time axis unline\n",
    "    typical Computer-Vision CutMix. Apply CutMix to one batch and its shifted version.\n",
    "    \"\"\"\n",
    "    def cutmix(specs, labels, alpha=alpha, prob=prob):\n",
    "        if random_float() > prob:\n",
    "            return specs, labels\n",
    "        spec_shape = tf.shape(specs)\n",
    "        label_shape = tf.shape(labels)\n",
    "        W = tf.cast(spec_shape[1], tf.int32)  # [batch, time, freq, channel]\n",
    "\n",
    "        # Lambda from beta distribution\n",
    "        beta = tfp.distributions.Beta(alpha, alpha)\n",
    "        lam = beta.sample(1)[0]\n",
    "\n",
    "        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n",
    "        specs_rolled = tf.roll(specs, shift=1, axis=0) # specs->[1, 2, 3], specs_rolled->[3, 1, 2]\n",
    "        labels_rolled = tf.roll(labels, shift=1, axis=0)\n",
    "\n",
    "        # Select random patch size\n",
    "        r_x = random_int([], minval=0, maxval=W)\n",
    "        r = 0.5 * tf.math.sqrt(1.0 - lam)\n",
    "        r_w_half = tf.cast(r * tf.cast(W, tf.float32), tf.int32)\n",
    "\n",
    "        # Select random location in time axis\n",
    "        x1 = tf.cast(tf.clip_by_value(r_x - r_w_half, 0, W), tf.int32)\n",
    "        x2 = tf.cast(tf.clip_by_value(r_x + r_w_half, 0, W), tf.int32)\n",
    "\n",
    "        # outer-pad patch -> [0, 0, x, x, 0, 0]\n",
    "        patch1 = specs[:, x1:x2, :, :]  # [batch, time, freq, channel]\n",
    "        patch1 = tf.pad(\n",
    "            patch1, [[0, 0], [x1, W - x2], [0, 0], [0, 0]])  # outer-pad\n",
    "\n",
    "        # inner-pad-patch -> [y, y, 0, 0, y, y]\n",
    "        patch2 = specs_rolled[:, x1:x2, :, :]  # [batch, mel, time, channel]\n",
    "        patch2 = tf.pad(\n",
    "            patch2, [[0, 0], [x1, W - x2], [0, 0], [0, 0]])  # outer-pad\n",
    "        patch2 = specs_rolled - patch2  # inner-pad-patch = img - outer-pad-patch\n",
    "\n",
    "        # patch1 -> [0, 0, x, x, 0, 0], patch2 -> [y, y, 0, 0, y, y]\n",
    "        # cutmix = (patch1 + patch2) -> [y, y, x, x, y, y]\n",
    "        specs = patch1 + patch2  # cutmix img\n",
    "\n",
    "        # Compute lambda = [1 - (patch_area/image_area)]\n",
    "        lam = tf.cast((1.0 - (x2 - x1) / (W)),tf.float32)  # no H term as (y1 - y2) = H\n",
    "        labels = lam * labels + (1.0 - lam) * labels_rolled  # cutmix label\n",
    "\n",
    "        specs = tf.reshape(specs, spec_shape)\n",
    "        labels = tf.reshape(labels, label_shape)\n",
    "\n",
    "        return specs, labels\n",
    "    return cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Spectrogram from audio\n",
    "def Audio2Spec(audio,spec_shape=[256, 128],sr=16000,nfft=2048,window=2048,fmin=20,fmax=8000):\n",
    "    spec_time = spec_shape[0]\n",
    "    spec_freq = spec_shape[1]\n",
    "    audio_len = tf.shape(audio)[0]\n",
    "    hop_length = tf.cast((audio_len // (spec_time - 1)), tf.int32) # compute hop_length to keep desired spec_shape\n",
    "    spec = tfio.audio.spectrogram(audio, nfft=nfft, window=window, stride=hop_length) # convert to spectrogram\n",
    "    mel_spec = tfio.audio.melscale(spec, rate=sr, mels=spec_freq, fmin=fmin, fmax=fmax) # transform to melscale\n",
    "    db_mel_spec = tfio.audio.dbscale(mel_spec, top_db=80) # from power to db (log10) scale\n",
    "    if tf.shape(db_mel_spec)[0] > spec_time:  # check if we have desiered shape\n",
    "        db_mel_spec = db_mel_spec[:spec_time,:]\n",
    "    db_mel_spec = tf.reshape(db_mel_spec, spec_shape)\n",
    "    return db_mel_spec\n",
    "\n",
    "# Convert spectrogram (H,W) to image (H,W,1)\n",
    "def Spec2Img(spec, num_channels=1):\n",
    "    # 1 channel image\n",
    "    img = spec[..., tf.newaxis]\n",
    "    # Copy same image across channel axis\n",
    "    if num_channels>1:\n",
    "        img = tf.tile(img, [1, 1, num_channels])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode audio from wav\n",
    "def decode_audio(data, audio_len):\n",
    "    # Decode\n",
    "    audio, sr = tf.audio.decode_wav(data)\n",
    "    audio = tf.reshape(audio, [audio_len]) # explicit size needed for TPU\n",
    "    audio = tf.cast(audio,tf.float32)\n",
    "    # Normalization\n",
    "    if CFG.normalize:\n",
    "        audio = Normalize(audio)\n",
    "    return audio\n",
    "\n",
    "# Decode label\n",
    "def decode_label(label):\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return label\n",
    "\n",
    "# Read tfrecord data & parse it & do augmentation\n",
    "def read_tfrecord(example, augment=True, return_id=False, return_label=True, target_len=CFG.audio_len, spec_shape=CFG.spec_shape):\n",
    "    tfrec_format = {\n",
    "        \"audio\" : tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"id\" : tf.io.FixedLenFeature([], tf.string),\n",
    "        \"speaker_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"system_id\" : tf.io.FixedLenFeature([], tf.string),\n",
    "        \"audio_len\" : tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"target\" : tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    # Parses a single example proto.\n",
    "    example = tf.io.parse_single_example(\n",
    "        example, tfrec_format\n",
    "    )\n",
    "    # Extract data from example proto\n",
    "    audio_id = example[\"id\"]\n",
    "    audio_len = example[\"audio_len\"]\n",
    "    # Decoding\n",
    "    audio = decode_audio(example[\"audio\"], audio_len)  # decode audio from .wav\n",
    "    target = decode_label(example[\"target\"]) # decode label -> type cast\n",
    "    # Trim Audio\n",
    "    audio = TrimAudio(audio)\n",
    "    # Crop or Pad audio to keep a fixed length\n",
    "    audio = CropOrPad(audio, target_len)\n",
    "    if augment:\n",
    "        # Apply AudioAug\n",
    "        audio = AudioAug(audio)\n",
    "    # Compute Spectrogram\n",
    "    spec = Audio2Spec(audio, spec_shape=spec_shape)\n",
    "    if augment:\n",
    "        # Apply SpecAug\n",
    "        spec = SpecAug(spec)\n",
    "    # Spectrogram (H, W) to Image (H, W, C)\n",
    "    img = Spec2Img(spec, num_channels=1)\n",
    "    # Clip & Reshape\n",
    "    img = tf.clip_by_value(img, 0, 1) if CFG.clip else img\n",
    "    img = tf.reshape(img, [*spec_shape, 1])\n",
    "\n",
    "    if not return_id:\n",
    "        if return_label:\n",
    "            return (img, target)\n",
    "        else:\n",
    "            return img\n",
    "    else:\n",
    "        if return_label:\n",
    "            return (img, target, audio_id)\n",
    "        else:\n",
    "            return (img, audio_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "        filenames,\n",
    "        shuffle=True,\n",
    "        repeat=True,\n",
    "        augment=True,\n",
    "        cache=True,\n",
    "        return_id=False,\n",
    "        return_label=True,\n",
    "        batch_size=CFG.batch_size * REPLICAS,\n",
    "        target_len=CFG.audio_len,\n",
    "        spec_shape=CFG.spec_shape,\n",
    "        drop_remainder=False,\n",
    "        seed=CFG.seed,\n",
    "):\n",
    "    # Real tfrecord files\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()  # cache data for speedup\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()  # repeat the data (for training only)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1024, seed=seed)  # shuffle the data (for training only)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_deterministic = False  # order won't be maintained when we shuffle\n",
    "        dataset = dataset.with_options(options)\n",
    "    # Parse data from tfrecord\n",
    "    dataset = dataset.map(lambda x: read_tfrecord(x,\n",
    "                                                  augment=augment,\n",
    "                                                  return_id=return_id,\n",
    "                                                  return_label=return_label,\n",
    "                                                  target_len=target_len,),\n",
    "                          num_parallel_calls=AUTO,)\n",
    "    # Batch Data Samples\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    # MixUp\n",
    "    if CFG.mixup_prob and augment and return_label:\n",
    "        dataset = dataset.map(get_mixup(alpha=CFG.mixup_alpha,prob=CFG.mixup_prob),num_parallel_calls=AUTO)\n",
    "    # CutMix\n",
    "    if CFG.cutmix_prob and augment and return_label:\n",
    "        dataset = dataset.map(get_cutmix(alpha=CFG.cutmix_alpha,prob=CFG.cutmix_prob),num_parallel_calls=AUTO)\n",
    "    # Prefatch data for speedup\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save = False):\n",
    "    \"\"\"Plot Confusion Matrix\"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    #     plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(False)\n",
    "    return\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\"Plot model training history\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(\n",
    "        np.arange(len(history[\"f1_score\"])),\n",
    "        history[\"f1_score\"],\n",
    "        \"-o\",\n",
    "        label=\"Train f1_score\",\n",
    "        color=\"#ff7f0e\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(len(history[\"f1_score\"])),\n",
    "        history[\"val_f1_score\"],\n",
    "        \"-o\",\n",
    "        label=\"Val f1_score\",\n",
    "        color=\"#1f77b4\",\n",
    "    )\n",
    "    x = np.argmax(history[\"val_f1_score\"])\n",
    "    y = np.max(history[\"val_f1_score\"])\n",
    "    xdist = plt.xlim()[1] - plt.xlim()[0]\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x, y, s=200, color=\"#1f77b4\")\n",
    "    plt.text(x - 0.03 * xdist, y - 0.13 * ydist, \"max f1_score\\n%.2f\" % y, size=14)\n",
    "    plt.ylabel(\"f1_score\", size=14)\n",
    "    plt.xlabel(\"Epoch\", size=14)\n",
    "    plt.legend(loc=2)\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(\n",
    "        np.arange(len(history[\"f1_score\"])),\n",
    "        history[\"loss\"],\n",
    "        \"-o\",\n",
    "        label=\"Train Loss\",\n",
    "        color=\"#2ca02c\",\n",
    "    )\n",
    "    plt2.plot(\n",
    "        np.arange(len(history[\"f1_score\"])),\n",
    "        history[\"val_loss\"],\n",
    "        \"-o\",\n",
    "        label=\"Val Loss\",\n",
    "        color=\"#d62728\",\n",
    "    )\n",
    "    x = np.argmin(history[\"val_loss\"])\n",
    "    y = np.min(history[\"val_loss\"])\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x, y, s=200, color=\"#d62728\")\n",
    "    plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n",
    "    plt.ylabel(\"Loss\", size=14)\n",
    "    plt.legend(loc=3)\n",
    "    plt.savefig(f\"history_plot.png\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def display_batch(batch, row=2, col=5):\n",
    "    \"Plot one batch data\"\n",
    "    imgs, tars = batch\n",
    "    plt.figure(figsize=(5.0*col, 3.5*row))\n",
    "    for idx in range(row*col):\n",
    "        img = imgs[idx].numpy().transpose()[0]\n",
    "        tar = tars[idx].numpy()\n",
    "        plt.subplot(row, col, idx+1)\n",
    "        plt.imshow(img, cmap='coolwarm')\n",
    "        text = 'Fake' if tar else 'Real'\n",
    "        plt.title(text, fontsize=15, color=('red' if tar else 'green'))\n",
    "    plt.tight_layout();\n",
    "    plt.grid(False)\n",
    "    plt.show();\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio\n",
    "ds = get_dataset(TRAIN_FILENAMES[:2], augment=False, cache=False, repeat=False).take(1)\n",
    "batch = next(iter(ds.unbatch().batch(20)))\n",
    "imgs, tars = batch\n",
    "print(f'image_shape: {imgs.shape} target_shape:{tars.shape}')\n",
    "print(f'image_dtype: {imgs.dtype} target_dtype:{tars.dtype}')\n",
    "display_batch(batch, row=3, col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset(TRAIN_FILENAMES[:2], augment=True, cache=False, repeat=False).take(1)\n",
    "batch = next(iter(ds.unbatch().batch(20)))\n",
    "imgs, tars = batch\n",
    "print(f'image_shape: {imgs.shape} target_shape:{tars.shape}')\n",
    "print(f'image_dtype: {imgs.dtype} target_dtype:{tars.dtype}')\n",
    "display_batch(batch, row=3, col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5, average='macro')\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    return [acc, precision, recall, f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(cfg):\n",
    "    lr_start   = cfg['LR_START']\n",
    "    lr_max     = cfg['LR_MAX'] * strategy.num_replicas_in_sync\n",
    "    lr_min     = cfg['LR_MIN']\n",
    "    lr_ramp_ep = cfg['LR_RAMPUP_EPOCHS']\n",
    "    lr_sus_ep  = cfg['LR_SUSTAIN_EPOCHS']\n",
    "    lr_decay   = cfg['LR_EXP_DECAY']\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audio_classification_models as acm\n",
    "model = acm.Conformer(input_shape=(128,80,1), pretrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audio_classification_models as acm\n",
    "\n",
    "URL = 'https://github.com/awsaf49/audio_classification_models/releases/download/v1.0.8/conformer-encoder.h5'\n",
    "\n",
    "def Conformer(input_shape=(128, 80, 1),num_classes=1, final_activation='sigmoid', pretrain=True):\n",
    "    \"\"\"Souce Code: https://github.com/awsaf49/audio_classification_models\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    backbone = acm.ConformerEncoder()\n",
    "    out = backbone(inp)\n",
    "    if pretrain:\n",
    "        acm.utils.weights.load_pretrain(backbone, url=URL)\n",
    "    out = tf.keras.layers.GlobalAveragePooling1D()(out)\n",
    "    #     out = tf.keras.layers.Dense(32, activation='selu')(out)\n",
    "    out = tf.keras.layers.Dense(num_classes, activation=final_activation)(out)\n",
    "    model = tf.keras.models.Model(inp, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name=CFG.model_name, loss=CFG.loss,):\n",
    "    model = Conformer(input_shape=[*CFG.spec_shape,1],pretrain=True)\n",
    "    lr = CFG.lr\n",
    "    if CFG.optimizer == \"Adam\":\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif CFG.optimizer == \"AdamW\":\n",
    "        opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=lr)\n",
    "    elif CFG.optimizer == \"RectifiedAdam\":\n",
    "        opt = tfa.optimizers.RectifiedAdam(learning_rate=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong Optimzer Name\")\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        steps_per_execution=CFG.steps_per_execution, # to reduce idle time\n",
    "        metrics=get_metrics()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy, CFG.device, tpu = configure_device()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = model.get_config() # Returns pretty much every information about your model\n",
    "# print(config[\"layers\"][0][\"config\"][\"batch_input_shape\"]) # returns a tuple of width, height and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.wandb:\n",
    "    \"login in wandb otherwise run anonymously\"\n",
    "    try:\n",
    "        # Addo-ons > Secrets > WANDB\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        api_key = user_secrets.get_secret(\"WANDB\")\n",
    "        wandb.login(key=api_key)\n",
    "        anonymous = None\n",
    "    except:\n",
    "        anonymous = \"must\"\n",
    "\n",
    "\n",
    "def wandb_init():\n",
    "    \"initialize project on wandb\"\n",
    "    id_ = wandb.util.generate_id() # generate random id\n",
    "    config = {k: v for k, v in dict(vars(CFG)).items() if \"__\" not in k} # convert class to dict\n",
    "    config[\"id\"] = id_\n",
    "    run = wandb.init(\n",
    "        id=id_,\n",
    "        project=\"fake-speech-detection\",\n",
    "        name=f\"dim-{CFG.spec_shape[0]}x{CFG.spec_shape[1]}|model-{CFG.model_name}\",\n",
    "        config=config,\n",
    "        anonymous=anonymous,\n",
    "        group=CFG.comment,\n",
    "        reinit=True,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb Run\n",
    "if CFG.wandb:\n",
    "    run = wandb_init()\n",
    "    WandbCallback = wandb.keras.WandbCallback(save_model=False)\n",
    "\n",
    "# Load gcs_path of train, valid & test\n",
    "TRAIN_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/train*.tfrec')\n",
    "VALID_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/valid*.tfrec')\n",
    "TEST_FILENAMES = tf.io.gfile.glob('/tmp/asvspoof/test*.tfrec')\n",
    "\n",
    "TRAIN_FILENAMES = TRAIN_FILENAMES[:2]\n",
    "VALID_FILENAMES = VALID_FILENAMES[:2]\n",
    "TEST_FILENAMES = TEST_FILENAMES[:2]\n",
    "\n",
    "# Take Only 10 Files if run in Debug Mode\n",
    "if CFG.debug:\n",
    "    TRAIN_FILENAMES = TRAIN_FILENAMES[:2]\n",
    "    VALID_FILENAMES = VALID_FILENAMES[:2]\n",
    "    TEST_FILENAMES = TEST_FILENAMES[:2]\n",
    "\n",
    "# Shuffle train files\n",
    "random.shuffle(TRAIN_FILENAMES)\n",
    "\n",
    "# Count train and valid samples\n",
    "NUM_TRAIN = count_data_items(TRAIN_FILENAMES)\n",
    "NUM_VALID = count_data_items(VALID_FILENAMES)\n",
    "NUM_TEST = count_data_items(TEST_FILENAMES)\n",
    "\n",
    "# Compute batch size & steps_per_epoch\n",
    "BATCH_SIZE = CFG.batch_size * REPLICAS\n",
    "STEPS_PER_EPOCH = NUM_TRAIN // BATCH_SIZE\n",
    "\n",
    "print(\"#\" * 60)\n",
    "print(\"#### IMAGE_SIZE: (%i, %i) | BATCH_SIZE: %i | EPOCHS: %i\"% (CFG.spec_shape[0],\n",
    "                                                                  CFG.spec_shape[1],\n",
    "                                                                  BATCH_SIZE,\n",
    "                                                                  CFG.epochs))\n",
    "print(\"#### MODEL: %s | LOSS: %s\"% (CFG.model_name, CFG.loss))\n",
    "print(\"#### NUM_TRAIN: {:,} | NUM_VALID: {:,}\".format(NUM_TRAIN, NUM_VALID))\n",
    "print(\"#\" * 60)\n",
    "\n",
    "# Log in w&B before training\n",
    "if CFG.wandb:\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"num_train\": NUM_TRAIN,\n",
    "            \"num_valid\": NUM_VALID,\n",
    "            \"num_test\": NUM_TEST,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Build model in device\n",
    "K.clear_session()\n",
    "with strategy.scope():\n",
    "    model = get_model(name=CFG.model_name,loss=CFG.loss)\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"/kaggle/working/ckpt.h5\",\n",
    "    verbose=CFG.verbose,\n",
    "    monitor=\"val_f1_score\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "# callbacks = [checkpoint, get_lr_callback(mode=CFG.lr_schedule,epochs=CFG.epochs)]\n",
    "\n",
    "# if CFG.wandb:\n",
    "    # Include w&b callback if WANDB is True\n",
    "    # callbacks.append(WandbCallback)\n",
    "\n",
    "# Create train & valid dataset\n",
    "train_ds = get_dataset(\n",
    "    TRAIN_FILENAMES,\n",
    "    augment=CFG.augment,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    cache=False,\n",
    "    drop_remainder=False,\n",
    ")\n",
    "valid_ds = get_dataset(\n",
    "    VALID_FILENAMES,\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    repeat=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    cache=False,\n",
    "    drop_remainder=False,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=CFG.epochs if not CFG.debug else 2,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    # callbacks=callbacks,\n",
    "    validation_data=valid_ds,\n",
    "    #         validation_steps = NUM_VALID/BATCH_SIZE,\n",
    "    verbose=CFG.verbose,\n",
    ")\n",
    "\n",
    "# Convert dict history to df history\n",
    "history = pd.DataFrame(history.history)\n",
    "\n",
    "# Load best weights\n",
    "model.load_weights(\"/kaggle/working/ckpt.h5\")\n",
    "\n",
    "# Plot Training History\n",
    "if CFG.display_plot:\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
